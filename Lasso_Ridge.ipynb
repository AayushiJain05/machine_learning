{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQnV3hdxll1Y",
        "outputId": "9159872d-3a6a-4b8b-ee97-d735f316b5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Regression equations ---\n",
            "\n",
            "Linear Regression :\n",
            "ŷ = -34.684058 + (9.109202*rm)\n",
            "\n",
            "Ridge Regression (alpha=1.000000):\n",
            "ŷ = -34.455009 + (9.072755*rm)\n",
            "\n",
            "Lasso Regression (alpha=1.000000):\n",
            "ŷ = -22.035919 + (7.096559*rm)\n",
            "\n",
            "--- Manual prediction arithmetic for first 10 rows ---\n",
            "\n",
            "Row 1:\n",
            "  rm=6.575000\n",
            "  Linear: -34.684058 + (9.109202 * 6.575000 = 59.893004) = 25.208946  (actual 24.000000)\n",
            "  Ridge : -34.455009 + (9.072755 * 6.575000 = 59.653361) = 25.198352  (actual 24.000000)\n",
            "  Lasso : -22.035919 + (7.096559 * 6.575000 = 46.659873) = 24.623954  (actual 24.000000)\n",
            "    Squared errors: Linear=1.461551, Ridge=1.436048, Lasso=0.389318\n",
            "\n",
            "Row 2:\n",
            "  rm=6.421000\n",
            "  Linear: -34.684058 + (9.109202 * 6.421000 = 58.490187) = 23.806129  (actual 21.600000)\n",
            "  Ridge : -34.455009 + (9.072755 * 6.421000 = 58.256157) = 23.801148  (actual 21.600000)\n",
            "  Lasso : -22.035919 + (7.096559 * 6.421000 = 45.567003) = 23.531084  (actual 21.600000)\n",
            "    Squared errors: Linear=4.867005, Ridge=4.845053, Lasso=3.729085\n",
            "\n",
            "Row 3:\n",
            "  rm=7.185000\n",
            "  Linear: -34.684058 + (9.109202 * 7.185000 = 65.449617) = 30.765559  (actual 34.700000)\n",
            "  Ridge : -34.455009 + (9.072755 * 7.185000 = 65.187742) = 30.732733  (actual 34.700000)\n",
            "  Lasso : -22.035919 + (7.096559 * 7.185000 = 50.988774) = 28.952855  (actual 34.700000)\n",
            "    Squared errors: Linear=15.479823, Ridge=15.739210, Lasso=33.029681\n",
            "\n",
            "Row 4:\n",
            "  rm=6.998000\n",
            "  Linear: -34.684058 + (9.109202 * 6.998000 = 63.746196) = 29.062139  (actual 33.400000)\n",
            "  Ridge : -34.455009 + (9.072755 * 6.998000 = 63.491136) = 29.036128  (actual 33.400000)\n",
            "  Lasso : -22.035919 + (7.096559 * 6.998000 = 49.661717) = 27.625798  (actual 33.400000)\n",
            "    Squared errors: Linear=18.817041, Ridge=19.043383, Lasso=33.341407\n",
            "\n",
            "Row 5:\n",
            "  rm=7.147000\n",
            "  Linear: -34.684058 + (9.109202 * 7.147000 = 65.103468) = 30.419410  (actual 36.200000)\n",
            "  Ridge : -34.455009 + (9.072755 * 7.147000 = 64.842977) = 30.387968  (actual 36.200000)\n",
            "  Lasso : -22.035919 + (7.096559 * 7.147000 = 50.719104) = 28.683185  (actual 36.200000)\n",
            "    Squared errors: Linear=33.415224, Ridge=33.779717, Lasso=56.502503\n",
            "\n",
            "Row 6:\n",
            "  rm=6.430000\n",
            "  Linear: -34.684058 + (9.109202 * 6.430000 = 58.572170) = 23.888112  (actual 28.700000)\n",
            "  Ridge : -34.455009 + (9.072755 * 6.430000 = 58.337812) = 23.882803  (actual 28.700000)\n",
            "  Lasso : -22.035919 + (7.096559 * 6.430000 = 45.630872) = 23.594953  (actual 28.700000)\n",
            "    Squared errors: Linear=23.154268, Ridge=23.205388, Lasso=26.061507\n",
            "\n",
            "Row 7:\n",
            "  rm=6.012000\n",
            "  Linear: -34.684058 + (9.109202 * 6.012000 = 54.764523) = 20.080465  (actual 22.900000)\n",
            "  Ridge : -34.455009 + (9.072755 * 6.012000 = 54.545400) = 20.090392  (actual 22.900000)\n",
            "  Lasso : -22.035919 + (7.096559 * 6.012000 = 42.664510) = 20.628591  (actual 22.900000)\n",
            "    Squared errors: Linear=7.949776, Ridge=7.893900, Lasso=5.159297\n",
            "\n",
            "Row 8:\n",
            "  rm=6.172000\n",
            "  Linear: -34.684058 + (9.109202 * 6.172000 = 56.221995) = 21.537938  (actual 27.100000)\n",
            "  Ridge : -34.455009 + (9.072755 * 6.172000 = 55.997041) = 21.542032  (actual 27.100000)\n",
            "  Lasso : -22.035919 + (7.096559 * 6.172000 = 43.799960) = 21.764041  (actual 27.100000)\n",
            "    Squared errors: Linear=30.936537, Ridge=30.891006, Lasso=28.472462\n",
            "\n",
            "Row 9:\n",
            "  rm=5.631000\n",
            "  Linear: -34.684058 + (9.109202 * 5.631000 = 51.293917) = 16.609859  (actual 16.500000)\n",
            "  Ridge : -34.455009 + (9.072755 * 5.631000 = 51.088681) = 16.633672  (actual 16.500000)\n",
            "  Lasso : -22.035919 + (7.096559 * 5.631000 = 39.960722) = 17.924802  (actual 16.500000)\n",
            "    Squared errors: Linear=0.012069, Ridge=0.017868, Lasso=2.030062\n",
            "\n",
            "Row 10:\n",
            "  rm=6.004000\n",
            "  Linear: -34.684058 + (9.109202 * 6.004000 = 54.691650) = 20.007592  (actual 18.900000)\n",
            "  Ridge : -34.455009 + (9.072755 * 6.004000 = 54.472818) = 20.017809  (actual 18.900000)\n",
            "  Lasso : -22.035919 + (7.096559 * 6.004000 = 42.607738) = 20.571819  (actual 18.900000)\n",
            "    Squared errors: Linear=1.226759, Ridge=1.249498, Lasso=2.794978\n",
            "\n",
            "\n",
            "--- Mean Squared Error (MSE) on entire dataset ---\n",
            "\n",
            "Linear MSE : 43.839575\n",
            "Ridge  MSE : 43.840235  (alpha=1.0)\n",
            "Lasso  MSE : 45.852218  (alpha=1.0)\n",
            "\n",
            "Best model by MSE: Linear (MSE = 43.839575)\n",
            "\n",
            "Summary saved to: results_summary.txt\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import textwrap\n",
        "import sys\n",
        "\n",
        "def detect_column(cols, key):\n",
        "    \"\"\"Case-insensitive lookup: return actual column name for key if found in cols\"\"\"\n",
        "    lower_map = {c.lower(): c for c in cols}\n",
        "    if key.lower() in lower_map:\n",
        "        return lower_map[key.lower()]\n",
        "    return None\n",
        "\n",
        "def fit_models(X, y, ridge_alpha=1.0, lasso_alpha=1.0):\n",
        "    lin = LinearRegression().fit(X, y)\n",
        "    ridge = Ridge(alpha=ridge_alpha).fit(X, y)\n",
        "    lasso = Lasso(alpha=lasso_alpha, max_iter=20000).fit(X, y)\n",
        "    return lin, ridge, lasso\n",
        "\n",
        "def model_eq_text(model, feature_names):\n",
        "    intercept = float(model.intercept_)\n",
        "    coefs = [float(c) for c in np.ravel(model.coef_)]\n",
        "    # Build equation string nicely\n",
        "    terms = []\n",
        "    for coef, fname in zip(coefs, feature_names):\n",
        "        terms.append(f\"({coef:.6f}*{fname})\")\n",
        "    rhs = \" + \".join(terms)\n",
        "    eq = f\"ŷ = {intercept:.6f} + {rhs}\"\n",
        "    return eq, intercept, coefs\n",
        "\n",
        "def manual_prediction_row(intercept, coefs, feature_names, row_values):\n",
        "    \"\"\"Return string showing arithmetic and predicted value\"\"\"\n",
        "    # row_values: list of values in same order as feature_names\n",
        "    parts = []\n",
        "    pred = intercept\n",
        "    parts.append(f\"{intercept:.6f}\")\n",
        "    for coef, fname, val in zip(coefs, feature_names, row_values):\n",
        "        prod = coef * val\n",
        "        pred += prod\n",
        "        parts.append(f\"({coef:.6f} * {val:.6f} = {prod:.6f})\")\n",
        "    arithmetic = \" + \".join(parts)\n",
        "    return arithmetic, pred\n",
        "\n",
        "def save_summary(path, lines):\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "    print(f\"\\nSummary saved to: {path}\")\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser(description=\"Linear / Ridge / Lasso regression demo (Boston housing)\")\n",
        "    p.add_argument(\"--csv\", default=\"BostonHousing.csv\", help=\"Path to CSV file (default BostonHousing.csv)\")\n",
        "    p.add_argument(\"--feature\", default=\"rm\", help=\"Feature column name (default rm). For multiple features, provide comma-separated names.\")\n",
        "    p.add_argument(\"--target\", default=\"medv\", help=\"Target column name (default medv)\")\n",
        "    p.add_argument(\"--n_manual\", type=int, default=10, help=\"Number of rows to show manual arithmetic for (default 10)\")\n",
        "    p.add_argument(\"--alpha\", type=float, default=1.0, help=\"Regularization alpha for Ridge & Lasso (default 1.0)\")\n",
        "    p.add_argument(\"--save\", default=\"results_summary.txt\", help=\"Summary output file (default results_summary.txt)\")\n",
        "    # Modify this line to parse an empty list of arguments when run in a notebook\n",
        "    # This prevents argparse from trying to parse internal Jupyter/Colab arguments.\n",
        "    args = p.parse_args([])\n",
        "\n",
        "    # Load CSV\n",
        "    try:\n",
        "        df = pd.read_csv(\"/content/BostonHousing.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: CSV file not found at '{args.csv}'. Provide correct path with --csv\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Determine feature(s) and target column names (case-insensitive)\n",
        "    feature_names_requested = [f.strip() for f in args.feature.split(\",\") if f.strip()]\n",
        "    resolved_feature_names = []\n",
        "    for f in feature_names_requested:\n",
        "        detected = detect_column(df.columns, f)\n",
        "        if detected is None:\n",
        "            print(f\"ERROR: feature column '{f}' not found in CSV columns: {list(df.columns)}\")\n",
        "            sys.exit(1)\n",
        "        resolved_feature_names.append(detected)\n",
        "\n",
        "    target_detected = detect_column(df.columns, args.target)\n",
        "    if target_detected is None:\n",
        "        # fallback to last column\n",
        "        target_detected = df.columns[-1]\n",
        "        print(f\"WARNING: target '{args.target}' not found; using last column '{target_detected}' as target.\")\n",
        "\n",
        "    feature_names = resolved_feature_names\n",
        "    target_name = target_detected\n",
        "\n",
        "    # Prepare data\n",
        "    use_cols = feature_names + [target_name]\n",
        "    data = df[use_cols].dropna().copy()\n",
        "    if data.shape[0] == 0:\n",
        "        print(\"No rows available after dropping NaNs. Exiting.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    X = data[feature_names].values\n",
        "    y = data[target_name].values\n",
        "\n",
        "    # Fit models\n",
        "    lin, ridge, lasso = fit_models(X, y, ridge_alpha=args.alpha, lasso_alpha=args.alpha)\n",
        "\n",
        "    # Equations\n",
        "    lin_eq, lin_inter, lin_coefs = model_eq_text(lin, feature_names)\n",
        "    ridge_eq, ridge_inter, ridge_coefs = model_eq_text(ridge, feature_names)\n",
        "    lasso_eq, lasso_inter, lasso_coefs = model_eq_text(lasso, feature_names)\n",
        "\n",
        "    # Print header\n",
        "    print(\"\\n--- Regression equations ---\\n\")\n",
        "    print(\"Linear Regression :\")\n",
        "    print(lin_eq)\n",
        "    print(\"\\nRidge Regression (alpha={:.6f}):\".format(args.alpha))\n",
        "    print(ridge_eq)\n",
        "    print(\"\\nLasso Regression (alpha={:.6f}):\".format(args.alpha))\n",
        "    print(lasso_eq)\n",
        "\n",
        "    # Manual predictions (first n_manual rows)\n",
        "    n_manual = min(args.n_manual, len(data))\n",
        "    sample = data.iloc[:n_manual].reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n--- Manual prediction arithmetic for first {n_manual} rows ---\\n\")\n",
        "    lines_out = []\n",
        "    lines_out.append(\"Regression Equations:\")\n",
        "    lines_out.append(\"Linear: \" + lin_eq)\n",
        "    lines_out.append(\"Ridge : \" + ridge_eq)\n",
        "    lines_out.append(\"Lasso : \" + lasso_eq)\n",
        "    lines_out.append(\"\")\n",
        "    lines_out.append(f\"Manual predictions for first {n_manual} rows (feature(s): {', '.join(feature_names)}; target: {target_name})\")\n",
        "    lines_out.append(\"\")\n",
        "\n",
        "    for i in range(n_manual):\n",
        "        row = sample.loc[i, feature_names]\n",
        "        actual = float(sample.loc[i, target_name])\n",
        "        row_vals = [float(row[f]) for f in feature_names]\n",
        "        print(f\"Row {i+1}:\")\n",
        "        vals_str = \", \".join([f\"{fname}={val:.6f}\" for fname, val in zip(feature_names, row_vals)])\n",
        "        print(\"  \" + vals_str)\n",
        "        lines_out.append(f\"Row {i+1}: {vals_str}  | actual {target_name} = {actual:.6f}\")\n",
        "\n",
        "        # Linear\n",
        "        arith_lin, pred_lin = manual_prediction_row(lin_inter, lin_coefs, feature_names, row_vals)\n",
        "        print(f\"  Linear: {arith_lin} = {pred_lin:.6f}  (actual {actual:.6f})\")\n",
        "        lines_out.append(f\"  Linear prediction: {arith_lin} = {pred_lin:.6f}  (actual {actual:.6f})\")\n",
        "\n",
        "        # Ridge\n",
        "        arith_ridge, pred_ridge = manual_prediction_row(ridge_inter, ridge_coefs, feature_names, row_vals)\n",
        "        print(f\"  Ridge : {arith_ridge} = {pred_ridge:.6f}  (actual {actual:.6f})\")\n",
        "        lines_out.append(f\"  Ridge prediction: {arith_ridge} = {pred_ridge:.6f}  (actual {actual:.6f})\")\n",
        "\n",
        "        # Lasso\n",
        "        arith_lasso, pred_lasso = manual_prediction_row(lasso_inter, lasso_coefs, feature_names, row_vals)\n",
        "        print(f\"  Lasso : {arith_lasso} = {pred_lasso:.6f}  (actual {actual:.6f})\")\n",
        "        lines_out.append(f\"  Lasso prediction: {arith_lasso} = {pred_lasso:.6f}  (actual {actual:.6f})\")\n",
        "\n",
        "        # squared errors (optional inclusion)\n",
        "        se_lin = (actual - pred_lin) ** 2\n",
        "        se_ridge = (actual - pred_ridge) ** 2\n",
        "        se_lasso = (actual - pred_lasso) ** 2\n",
        "        print(f\"    Squared errors: Linear={se_lin:.6f}, Ridge={se_ridge:.6f}, Lasso={se_lasso:.6f}\\n\")\n",
        "        lines_out.append(f\"    Squared errors: Linear={se_lin:.6f}, Ridge={se_ridge:.6f}, Lasso={se_lasso:.6f}\\n\")\n",
        "\n",
        "    # Compute MSE on the entire dataset\n",
        "    preds_lin = lin.predict(X)\n",
        "    preds_ridge = ridge.predict(X)\n",
        "    preds_lasso = lasso.predict(X)\n",
        "\n",
        "    mse_lin = mean_squared_error(y, preds_lin)\n",
        "    mse_ridge = mean_squared_error(y, preds_ridge)\n",
        "    mse_lasso = mean_squared_error(y, preds_lasso)\n",
        "\n",
        "    print(\"\\n--- Mean Squared Error (MSE) on entire dataset ---\\n\")\n",
        "    print(f\"Linear MSE : {mse_lin:.6f}\")\n",
        "    print(f\"Ridge  MSE : {mse_ridge:.6f}  (alpha={args.alpha})\")\n",
        "    print(f\"Lasso  MSE : {mse_lasso:.6f}  (alpha={args.alpha})\")\n",
        "\n",
        "    lines_out.append(\"MSE (entire dataset):\")\n",
        "    lines_out.append(f\"Linear: {mse_lin:.6f}\")\n",
        "    lines_out.append(f\"Ridge : {mse_ridge:.6f} (alpha={args.alpha})\")\n",
        "    lines_out.append(f\"Lasso : {mse_lasso:.6f} (alpha={args.alpha})\")\n",
        "\n",
        "    # Best model by MSE\n",
        "    mse_dict = {\"Linear\": mse_lin, \"Ridge\": mse_ridge, \"Lasso\": mse_lasso}\n",
        "    best = min(mse_dict, key=mse_dict.get)\n",
        "    print(f\"\\nBest model by MSE: {best} (MSE = {mse_dict[best]:.6f})\")\n",
        "    lines_out.append(\"\")\n",
        "    lines_out.append(f\"Best model by MSE: {best} (MSE = {mse_dict[best]:.6f})\")\n",
        "    lines_out.append(\"\")\n",
        "    lines_out.append(\"Notes for observation book:\")\n",
        "    lines_out.append(\" - Record the regression equations (intercept and coefficients).\")\n",
        "    lines_out.append(\" - For each sample row, write the arithmetic: ŷ = intercept + sum(coef_i * feature_i).\")\n",
        "    lines_out.append(\" - Show one or two squared-error calculations then compute MSE using the formula:\")\n",
        "    lines_out.append(\"     MSE = (1/n) * sum_{i=1..n} (y_i - ŷ_i)^2\")\n",
        "    lines_out.append(\" - Conclude which model is best by lowest MSE. You may vary alpha and repeat to observe changes.\")\n",
        "\n",
        "    # Save summary to file for upload/printing\n",
        "    save_summary(args.save, lines_out)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}